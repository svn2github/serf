<gstein> btw, on serf, I was thinking this morning about defining "endpoints" or "data generators"
<jerenkrantz> k
<gstein> when you're operating in a "push" model, then the data is implicit,
<gstein> the app is performing its main loop and shoving data into the filter chains
<gstein> however, on the "pull" style, 
<gstein> an app says "get me 50 bytes from <that> filter"
<gstein> the system needs to talk to <somethign> to push bytes into the bottom of the filter chain,
<jerenkrantz> wouldn't it pull from a socket?
<gstein> while it has a thing installed at the other end to collect the output,
<gstein> when it reaches 50 bytes, then it stops telling the bottom to shove bytes, and returns the 50 bytes; setting aside any extra that got pushed out of the chain
<gstein> to be specific: one data generator is a socket, yes
<gstein> so when the app says "give me 50 bytes",
<gstein> serf says "yo, socket. grab some data and shove it into the chain"
<gstein> serf catches the spill out of the end of the chain, buffering it
<jerenkrantz> and it stops at 50
<gstein> well, it has to catch everything that spills out
<jerenkrantz> right - it'd have to do buffering itself.
<gstein> yup
<gstein> brigade->setaside() really
<jerenkrantz> but, that's okay.  the app shouldn't get more than 50 bytes
<gstein> depends
<gstein> the socket could say "you want 50? well, I'll grab 50" but it could expand to 80
<jerenkrantz> it wouldn't return 80 would it?
<gstein> so the socket returns, serf picks up 50 from the buffered data, and returns it
* sander-afk notes that each filter should catch its own spill
<gstein> nah
<sander-afk> cache
<gstein> sander-afk: this is in "pull" mode
<sander-afk> Hmm
<sander-afk> Ok
<gstein> given a chain, an app says "yo. chain. give me 50 bytes."
* sander-afk retreats back to the living room then with his phone...
<gstein> the app should not get 80.
<gstein> that might be too much.
<sander-afk> Sure, it shouldn't.
<jerenkrantz> my point.
<jerenkrantz> but, next time if it asks for 30, it's (essentially) free
<gstein> yup :-)
<sander-afk> But the filter expanding the 50 -> 80, should give the 50 first expanded bytes
<sander-afk> and cache the 30 left for next call
<gstein> but the filter doesn't know the requirements
<gstein> recall that the filters are all designed as push filters
<gstein> 50 comes in, it pushes 80
<jerenkrantz> so, serf has to be what the calls.  it can't call the filters directly.  that's the mistake with httpd.
<jerenkrantz> (what the app calls)
<sander-afk> Ah
<sander-afk> We need a layer on top of the filters
<sander-afk> ?
<sander-afk> Right
<gstein> *if* you operate in pull mode, yes
<jerenkrantz> sander-afk: i think so
<gstein> in push mode, no
<sander-afk> We could implement it as a caching filter
<jerenkrantz> push mode would be called from some socket poll() loop pushing it on to the bottom of the chain?
<sander-afk> Which would only be used in pull mode
<gstein> well, that filter would need to coordinate heavily with serf as serf works to do the pull semantics
* sander-afk nod
<sander-afk> s
<gstein> but yes: it would be a filter plugged onto the end of the chain to catch the pushed data
<gstein> jerenkrantz: yes
<gstein> in push mode, there isn't a "data generator" concept
<gstein> the app itself is feeding the chain
<gstein> in the pull mode, serf needs to talk to somebody to feed the chain
<gstein> so that <somebody> is defined as a "data generator" (or somesuch)
<jerenkrantz> gstein: right on the push-mode we don't care how much we output.
<gstein> despite my inclination for an http-only system, I can't help but define the I/O chains as abstractions :-)
<gstein> output from the chain? yes. we can't even control what comes out.
<gstein> but we can set aside the excess
<gstein> and that is why pull sux relative to push
<jerenkrantz> right.  ideally the app has something at the top of the filter?  (do we want to setaside the excess?)
<gstein> push is always the ideal scenario
<jerenkrantz> oh crap.
<jerenkrantz> yeah, an app could do push/pull, no?
<jerenkrantz> yeah, we'd have to setaside all output on a push.
<gstein> serf installs a (hidden) filter to capture excess
<-- sabiHomeHomeHome has quit (Read error: 104 (Connection reset by peer))
<gstein> yup. *all* data
<gstein> hmm. not all.
<gstein> but we would have to *copy* it all
<jerenkrantz> whatever comes out of the top of the filter
<gstein> get_data(buf, bufsize)
<gstein> the data spills out of the chain. some portion is copied to the buffer. some is setaside.
--> Sebast1an (~sb@p3E9E3653.dip.t-dialin.net) has joined #apr
<jerenkrantz> to what buffer?
<gstein> and yes, an app can choose to do push or pull. we don't know its operational model, so we support both
<gstein> and if all filters are designed as push filters, then we can do both
<jerenkrantz> right.
<gstein> get_data(my_buffer, my_buffer_size)
<jerenkrantz> a pull just triggers a call to the socket to go read and push
<gstein> the spill filter copies some to my_buffer
<gstein> yes
--> sabiHomeHomeHome (~nicholas@nicholasriley.ne.client2.attbi.com) has joined #apr
<gstein> I've been calling that socket a "data generator", in the abstract sense that you could have anything feeding the filter chain
<jerenkrantz> right
<jerenkrantz> but, get_data may not be called immediately.
<jerenkrantz> here's what i'm thinking:
<gstein> hm? no no..
<gstein> get_data() is called by the app to pull data
<jerenkrantz> ok
<jerenkrantz> yes
<gstein> underneat, serf calls the genrator to push data into the chain, and catches the output in a "spill" filter
<jerenkrantz> and returns it.
<jerenkrantz> that's not the case i'm thinking of though.
<gstein> copying a porting ot the provided buffer, and setting aside the excess (for the next call)
<gstein> ok
<gstein> ?
<jerenkrantz> the case i'm thinking of is when a push occurs - what happens with the end result?
<gstein> normal push operation?
<jerenkrantz> 'cuz then the app could come along *later* and do a pull?
<gstein> oh no
<jerenkrantz> yes, normal push followed by a pull.
<gstein> I think a chain is set up for one direction only
<jerenkrantz> okay, i guess i can buy that.
<gstein> it doesn't seem like an app would dynamically change how it drives a chain
<jerenkrantz> so, your writes are push with a socket at the end.  and reads are pulls with a socket at the beginning
<gstein> easy to legislate against that :-)
<gstein> no :-)
<gstein> IMO, the app never reads
<gstein> the socket reads and pushes data into a chain. the app catches it and processes it
<jerenkrantz> right, it should use filters?
<jerenkrantz> well, in a client - the client has to write first
<gstein> yes. the client pushes data into the output chain.
<gstein> the poll loop gets that data delivered out the socket, but also reads data.
<gstein> as the data arrives, it is pushed into the input chain.
<jerenkrantz> for a server, you're right where the app could be entirely done without a main loop.
<gstein> the app has a filter at the end of the input chain to process that incoming data.
<jerenkrantz> right,
<gstein> however, it is a bit tricky to coordinate writes and reads
<jerenkrantz> httpd could go towards this model and filters along the way setup the request and start writing it back.
<gstein> a write into the output chain blocks.
<jerenkrantz> yes
<gstein> thus, if you're processing input, then you need to be a bit careful about blocking.
<gstein> (presuming a single-threaded poll loop)
<jerenkrantz> on a push-model, you'd only read as much as-is available, no?
<gstein> the problem is that you'd never return to the poll loop to deliver the data
<gstein> correct
<-- Sebast1an has quit ("Client Exiting")
<jerenkrantz> ah, you'd have to interrupt the poll.
<jerenkrantz> no biggie there - two poll()s - each one independent?
<gstein> ideally, you'd have a thread deliver the read data
<jerenkrantz> could
<jerenkrantz> do you mean "thread deliver the read data to *your* thread?"
<gstein> incoming data gets its own thread
<gstein> cuz that thread may block on a write
<gstein> and you don't want to block further reads
<jerenkrantz> but there's definite state-requirements...you can't have mult reads on one socket be on diff threads?
<gstein> for example, you receive 10 bytes. you write 1k, but the OS blocks you at 100. the other end doesn't read past 100 cuz it just sent you another 10k. you have to consume the 10k before the server unblocks and you can write past the 100.
<gstein> a socket bucket should have a size associated with it.
<gstein> "here is a socket. you can read up to 10 bytes."
<gstein> the socket mutates to nothing after those 10 bytes.
<gstein> the socket bucket can also have a dependency on another socket bucket. "you can't read until *that* bucket has read his N bytes."
<jerenkrantz> that can't work - think about ssl.  it has a state and has negotiation requirements.  (read, write, read more, etc, etc) - the filter state would have to be transfered to each filter?
<jerenkrantz> err, read thread
<gstein> um. what can't work?
<jerenkrantz> 10 bytes come in.  new thread starts.
<jerenkrantz> we have ssl filter setup...
<jerenkrantz> so it reads the 10 bytes and then writes what it needs to for the SSL negotiation to the socket.
<jerenkrantz> then, the filter needs to read the response *back* from the other end and then continue processing.
<gstein> another thread is going to get that response
<jerenkrantz> it can't.  how does the ssl filter know where to pick up?
<gstein> cuz T1 is going to comlete the write and unwind back to the poll loop and die.
<gstein> yes, a state machine :-)
<gstein> in any async system, you need a state machine
* jerenkrantz groans
<gstein> you can use pull semantics to avoid a state machine
<jerenkrantz> problem is openssl is going to kill us on this as it doesn't expose a state machine.
<gstein> but if you're using *push*, then you must have a state machine
<gstein> cuz you could be pushed 1 byte at a time
<gstein> (let alone the whole block if you do a write back to the output chain)
<gstein> the app can easily push data at the output chain, and read from the input, and avoid state machines
<gstein> but you're going to get copies on the pull
<gstein> hmm...
<gstein> well... I guess we could return a size-limited brigade
<gstein> (e.g. the size-limited socket)
<jerenkrantz> think about mod_include being pushed...it has to know what the buffer before was - a state machine isn't expressive enough, is it?
<gstein> right now, we don't have size-limited sockets, so apache cannot do zero-copy up to the handler. when it returns a brigade (via ap_get_brigade()), it cannot returna socket bucket.
<jerenkrantz> <!--#ec*ugh - nothing more - stop*
<gstein> mod_include has a state machine
<jerenkrantz> next thread comes: ho....
<jerenkrantz> hmm
<jerenkrantz> i guess a concept like our current filter context would be enough
<gstein> see mod_include.h, line 134. the 'states' enum.
<gstein> push-based filters (and the chain endpoint) need to be state machines
<gstein> (depending on what the heck they're doing, of course)
<gstein> but a pull system doesn't need to be
* jerenkrantz tries to envision deflate as state machine
<gstein> zlib has it internally
<gstein> we just shove data at zlib
<jerenkrantz> yeah, as long as we get the context, we should be okay
<jerenkrantz> but, no two threads are going to be operating on the same context.  once a filter does a write, that thread is kaput.
<gstein> sure. it is expected that is where your state exists :-)
<gstein> well... no
<jerenkrantz> um?
<gstein> poll loop sends data into a filter chain on thread T1
<jerenkrantz> oh the dependency on the socket you mentioned earlier?
<gstein> the data arrives at the end of the chain; T1 does processing; T1 writes to the output; T1 blocks
<jerenkrantz> k
<gstein> more data arrives. T2 is spun up. T1 read everything from the socket, so T2 is free to begin reading the socket. it does so.
<gstein> somewhere in there, T1's write completes, and it unblocks and unwinds.
<gstein> but both T1 and T2 are in the same code area that can read a specific context.
<jerenkrantz> but, the contexts of the filters has to be synchronized, no?
<gstein> yes :-)
<gstein> however!
<gstein> that is *entirely* dependent upon the architecture of the poll loop
<gstein> when a write arrives, the output chain's endpoint could setaside the data and return
<gstein> when T1 returns to the poll loop, *then* it could write the data
<jerenkrantz> rather than blocking T1.
<gstein> right
<jerenkrantz> but, the key concept is still when T1 does a write, it can't do anything more (other than wait for the write to really occur or to be setaside)
<gstein> alternatively, the poll loop could say "this socket is ready for writing. Mr Output Chain: give me 8k of data."
<jerenkrantz> i see
<gstein> yes
<gstein> the push model is also nice because it can throttle
<gstein> when you shove 10 meg of data into the output chain, the network is going to push back on you to slow down to match the remote host's capability
<gstein> if the poll loop did a setaside, you would now have 10 meg pending
<gstein> depending on the structure of that 10 meg, it could be expensive as hell to do the setaside
<gstein> damn. i should write all this down :-)
<jerenkrantz> i'm trying to.
<jerenkrantz> and failing miserably
<gstein> hm? you're taking notes from this convo?
<jerenkrantz> yea.  rough
<gstein> for committing to the serf repos?
<jerenkrantz> it'd need to be *really* polished.  it's basically so when i wake up, i have some idea what we talked about
* gstein chuckles
<jerenkrantz> okay, so if we have 10MB that we want to write, we'll keep trying write it as fast as we can, but since we're in the poll() loop, we'll also catch any incoming reads and start to process them as well.  any data that these (new) threads produce get shoved at the end of the 10mb (or what is left)
<jerenkrantz> sounds like nice pipelining
<gstein> hmm. output.
* gstein ponders
<gstein> that one is a bit more difficult
<gstein> in terms of interlocking the two write operations to serialize them
<gstein> hm
<gstein> the endpoint filter that does the write could easily see the thread ID making the write
<gstein> it could then serialize the writes relative to the arrival of the write request on that chain
<jerenkrantz> eek.
<gstein> but how to deal with "T1 hasn't returned yet, so I'm assuming it is writing, so you must block on T1"
<jerenkrantz> out-of-order problems.
<gstein> right
<gstein> reading is much easier than writing...
<gstein> only one guy does the read, so he can order stuff
<gstein> but many people do a write
<gstein> so you need a rule
<gstein> any old rule, but a rule nonetheless :-)
<gstein> "first come first served" would be the easiest
<jerenkrantz> but incorrect me thinks
<gstein> you could also do various thread-based things.
<jerenkrantz> (well, when the underlying protocol doesn't support out-of-order.  waka very well might.)
<gstein> well, if a write is in *progress*, then your ordering is okay
* gstein nods
<gstein> e.g. the filter is presented with the full 10Mb
<jerenkrantz> but, T1 could still be processing something expensive.  T2's request is simple.  T2 really can't write until T1 has completed
<gstein> all further writes are shoved behind that
<gstein> unknown, actually
<gstein> T1 could be done with the socket and is simply logging stuff to a remote database
<gstein> T2 should be able to write to the socket at that point
<jerenkrantz> so it needs to have some sort of mutex lock on the socket?
<gstein> however, if T1 is midstream on processing a series of ops from a read request, then you don't want T2 interjecting crap
<gstein> a mutex for the mechanics, sure,
<gstein> but the semantics... hoo
<jerenkrantz> T1 sends some type of message when it is done with the socket?
* jerenkrantz considers if condvars can help
<jerenkrantz> one thing you could do is enforce all threads to send a pseudo-EOS when they are done with the socket.
<gstein> where to put them is the question
<gstein> RELEASE :-)
<jerenkrantz> ah, yeah
<gstein> yah
<gstein> hmm. that might do it
<gstein> optimization only, I think
<gstein> but a seriously *useful* one
